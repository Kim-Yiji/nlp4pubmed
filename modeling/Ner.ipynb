{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d80ee78",
   "metadata": {},
   "source": [
    "#BIO í¬ë§·ìœ¼ë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd091c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BIO í¬ë§· ë³€í™˜ ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: Database/NER/ner_bio_format_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. BioBERT í† í¬ë‚˜ì´ì € ë¡œë”©\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "# 2. íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "input_path = \"Database/NER/gpt4_ner_results.jsonl\"\n",
    "output_path = \"Database/NER/ner_bio_format_results.jsonl\"\n",
    "\n",
    "# 3. ì‚¬ìš©í•  ì—”í‹°í‹° íƒœê·¸ ì¢…ë¥˜\n",
    "entity_labels = [\"INGREDIENT\", \"SYMPTOM\", \"DOSAGE\", \"SENSITIVE_CONDITION\", \"PERSONAL_INFO\"]\n",
    "\n",
    "# 4. BIO íƒœê¹… í•¨ìˆ˜\n",
    "def convert_to_bio(text, entities):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    labels = ['O'] * len(tokens)\n",
    "\n",
    "    for ent_type in entity_labels:\n",
    "        for phrase in entities.get(ent_type, []):\n",
    "            phrase_tokens = tokenizer.tokenize(phrase)\n",
    "            for i in range(len(tokens) - len(phrase_tokens) + 1):\n",
    "                if tokens[i:i + len(phrase_tokens)] == phrase_tokens:\n",
    "                    labels[i] = f'B-{ent_type}'\n",
    "                    for j in range(1, len(phrase_tokens)):\n",
    "                        labels[i + j] = f'I-{ent_type}'\n",
    "                    break  # í•˜ë‚˜ë§Œ ì²˜ë¦¬í•˜ê³  ë\n",
    "\n",
    "    return {\"tokens\": tokens, \"labels\": labels}\n",
    "\n",
    "# 5. ì…ë ¥ JSONL ì½ê³  ë³€í™˜\n",
    "bio_data = []\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            item = json.loads(line)\n",
    "            if \"text\" in item and \"entities\" in item:\n",
    "                bio_entry = convert_to_bio(item[\"text\"], item[\"entities\"])\n",
    "                bio_data.append(bio_entry)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing line: {e}\")\n",
    "\n",
    "# 6. BIO JSONL ì €ì¥ (í•œ ì¤„ì”© JSON ê°ì²´ë¡œ)\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in bio_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "print(f\"âœ… BIO í¬ë§· ë³€í™˜ ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3ff6c",
   "metadata": {},
   "source": [
    "# finetuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42caed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /opt/anaconda3/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/anaconda3/lib/python3.12/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71ff08f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchcrf in /opt/anaconda3/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchcrf) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchcrf) (2.3.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.0.0->torchcrf) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.0.0->torchcrf) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchcrf\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c8ce9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe79956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchcrf in /opt/anaconda3/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchcrf) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchcrf) (2.3.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.0.0->torchcrf) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.0.0->torchcrf) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torchcrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f23fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.2\n",
      "zsh:1: parse error near `-m'\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "print(pip.__version__)\n",
    "!{sys.executable} -m pip list | grep torchcrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9aced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchcrf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, f1_score, precision_score, recall_score\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchcrf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CRF\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchcrf'"
     ]
    }
   ],
   "source": [
    "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import json\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, Trainer, TrainingArguments,\n",
    "    DataCollatorForTokenClassification, EarlyStoppingCallback\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from torch import nn\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd66609",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchcrf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, f1_score, precision_score, recall_score\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchcrf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CRF\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 2. ë°ì´í„° ë¡œë“œ\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatabase/NER/ner_bio_format_results.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchcrf'"
     ]
    }
   ],
   "source": [
    "# 2. ë°ì´í„° ë¡œë“œ\n",
    "with open(\"Database/NER/ner_bio_format_results.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# 3. ë¼ë²¨ ì •ì˜ ë° ë§¤í•‘\n",
    "label_list = [\n",
    "    \"O\", \"B-INGREDIENT\", \"I-INGREDIENT\", \"B-DOSAGE\", \"I-DOSAGE\",\n",
    "    \"B-SYMPTOM\", \"I-SYMPTOM\", \"B-SENSITIVE_CONDITION\", \"I-SENSITIVE_CONDITION\",\n",
    "    \"B-PERSONAL_INFO\", \"I-PERSONAL_INFO\"\n",
    "]\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# 4. í† í¬ë‚˜ì´ì €\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "# 5. train/test ë¶„í• \n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. ì¸ì½”ë”© + ë ˆì´ë¸” ì •ë ¬ í•¨ìˆ˜\n",
    "def encode_and_align_labels(data):\n",
    "    tokens = [d['tokens'] for d in data]\n",
    "    labels = [[label_to_id[tag] for tag in d['labels']] for d in data]\n",
    "    encodings = tokenizer(tokens, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "    encodings.pop(\"offset_mapping\")\n",
    "\n",
    "    labels_aligned = []\n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = encodings.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label[word_idx] % 2 == 1 else label[word_idx] + 1)\n",
    "            previous_word_idx = word_idx\n",
    "        labels_aligned.append(label_ids)\n",
    "    return encodings, labels_aligned\n",
    "\n",
    "train_encodings, train_labels = encode_and_align_labels(train_data)\n",
    "test_encodings, test_labels = encode_and_align_labels(test_data)\n",
    "\n",
    "# 7. Dataset ì •ì˜\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NERDataset(train_encodings, train_labels)\n",
    "test_dataset = NERDataset(test_encodings, test_labels)\n",
    "\n",
    "# 8. CRF ëª¨ë¸ ì •ì˜\n",
    "class BioBERT_CRF(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        emissions = self.classifier(sequence_output)\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(emissions, labels, mask=attention_mask.bool(), reduction='mean')\n",
    "            return {\"loss\": loss, \"logits\": emissions}\n",
    "        else:\n",
    "            pred = self.crf.decode(emissions, mask=attention_mask.bool())\n",
    "            return {\"logits\": pred}\n",
    "\n",
    "model = BioBERT_CRF(\"dmis-lab/biobert-base-cased-v1.1\", num_labels=len(label_list))\n",
    "\n",
    "# 9. í‰ê°€ ë©”íŠ¸ë¦­ ì •ì˜\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    if isinstance(predictions[0][0], int):  # CRF decode ê²°ê³¼\n",
    "        decoded_preds = predictions\n",
    "    else:\n",
    "        decoded_preds = predictions.argmax(axis=-1)\n",
    "\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "    for pred, label in zip(decoded_preds, labels):\n",
    "        pred_label = []\n",
    "        true_label = []\n",
    "        for p_i, l_i in zip(pred, label):\n",
    "            if l_i != -100:\n",
    "                pred_label.append(label_list[p_i])\n",
    "                true_label.append(label_list[l_i])\n",
    "        true_preds.append(pred_label)\n",
    "        true_labels.append(true_label)\n",
    "\n",
    "    print(\"\\n\" + classification_report(true_labels, true_preds))\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_preds),\n",
    "        \"recall\": recall_score(true_labels, true_preds),\n",
    "        \"f1\": f1_score(true_labels, true_preds)\n",
    "    }\n",
    "\n",
    "# 10. í•™ìŠµ ì„¤ì • (early stopping í¬í•¨)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./biobert_ner_output\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# 11. Trainer ì •ì˜ + EarlyStopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# 12. í•™ìŠµ ë° ì €ì¥\n",
    "trainer.train()\n",
    "trainer.save_model(\"./biobert_ner_model\")\n",
    "print(\"âœ… í•™ìŠµ ë° ëª¨ë¸ ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# 13. ì˜ˆì¸¡ í›„ì²˜ë¦¬ ì˜ˆì‹œ\n",
    "predictions = trainer.predict(test_dataset).predictions\n",
    "decoded_preds = predictions if isinstance(predictions[0][0], int) else predictions.argmax(axis=-1)\n",
    "\n",
    "print(\"\\nğŸ¯ ì˜ˆì¸¡ ê²°ê³¼ ì˜ˆì‹œ\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n[ë¬¸ì¥ {i+1}]\")\n",
    "    result = []\n",
    "    for p_i, l_i in zip(decoded_preds[i], test_labels[i]):\n",
    "        if l_i != -100:\n",
    "            result.append(f\"PRED: {label_list[p_i]:<25} | TRUE: {label_list[l_i]}\")\n",
    "    print(\"\\n\".join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3684969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def merge_tokens(tokens):\n",
    "    text = \"\"\n",
    "    for tok in tokens:\n",
    "        if tok.startswith(\"##\"):\n",
    "            text += tok[2:]\n",
    "        elif text:\n",
    "            text += \" \" + tok\n",
    "        else:\n",
    "            text = tok\n",
    "    return text.strip()\n",
    "\n",
    "def recover_entities(tokens, labels):\n",
    "    entities = {\n",
    "        \"INGREDIENT\": [],\n",
    "        \"SYMPTOM\": [],\n",
    "        \"DOSAGE\": [],\n",
    "        \"SENSITIVE_CONDITION\": [],\n",
    "        \"PERSONAL_INFO\": []\n",
    "    }\n",
    "    current_tokens = []\n",
    "    current_type = None\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label == \"O\":\n",
    "            if current_tokens:\n",
    "                entities[current_type].append(merge_tokens(current_tokens))\n",
    "                current_tokens = []\n",
    "                current_type = None\n",
    "        elif label.startswith(\"B-\"):\n",
    "            if current_tokens:\n",
    "                entities[current_type].append(merge_tokens(current_tokens))\n",
    "            current_type = label[2:]\n",
    "            current_tokens = [token]\n",
    "        elif label.startswith(\"I-\") and current_type == label[2:]:\n",
    "            current_tokens.append(token)\n",
    "        else:\n",
    "            if current_tokens:\n",
    "                entities[current_type].append(merge_tokens(current_tokens))\n",
    "            current_tokens = []\n",
    "            current_type = None\n",
    "\n",
    "    if current_tokens:\n",
    "        entities[current_type].append(merge_tokens(current_tokens))\n",
    "\n",
    "    return entities\n",
    "\n",
    "def convert_bio_json_to_rag_jsonl(input_path, output_path=\"Database/NER/metadata_converted.jsonl\"):\n",
    "    rag_data = []\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            tokens = item[\"tokens\"]\n",
    "            labels = item[\"labels\"]\n",
    "            text = merge_tokens(tokens)\n",
    "            meta = recover_entities(tokens, labels)\n",
    "            rag_data.append({\n",
    "                \"text\": text,\n",
    "                \"meta\": meta\n",
    "            })\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in rag_data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… ë³€í™˜ ì™„ë£Œ: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "655fbcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë³€í™˜ ì™„ë£Œ: Database/NER/metadata_converted.jsonl\n"
     ]
    }
   ],
   "source": [
    "convert_bio_json_to_rag_jsonl(\"Database/NER/ner_bio_format_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd932a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
