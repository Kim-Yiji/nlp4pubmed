{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.32.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.32.2-py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, pyyaml, hf-xet, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.5.1 hf-xet-1.1.2 huggingface-hub-0.32.2 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.52.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 분류된 정답 데이터 기반 fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-20T07:05:11.140336Z",
     "iopub.status.busy": "2025-05-20T07:05:11.140051Z",
     "iopub.status.idle": "2025-05-20T07:05:11.146999Z",
     "shell.execute_reply": "2025-05-20T07:05:11.146227Z",
     "shell.execute_reply.started": "2025-05-20T07:05:11.140315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 데이터 준비\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T07:05:13.739771Z",
     "iopub.status.busy": "2025-05-20T07:05:13.739058Z",
     "iopub.status.idle": "2025-05-20T07:05:13.743741Z",
     "shell.execute_reply": "2025-05-20T07:05:13.742921Z",
     "shell.execute_reply.started": "2025-05-20T07:05:13.739743Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. 모델 설정\n",
    "def setup_model(num_labels):\n",
    "    # BioBERT 모델과 토크나이저 로드\n",
    "    model_name = \"dmis-lab/biobert-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T08:38:30.521748Z",
     "iopub.status.busy": "2025-05-20T08:38:30.521178Z",
     "iopub.status.idle": "2025-05-20T08:38:30.533562Z",
     "shell.execute_reply": "2025-05-20T08:38:30.532826Z",
     "shell.execute_reply.started": "2025-05-20T08:38:30.521727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=10, label_names=None):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.to(device)\n",
    "    best_val_accuracy = 0.0  # 최고 accuracy 추적\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 학습\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in train_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{(train_correct/train_total)*100:.2f}%'\n",
    "            })\n",
    "        \n",
    "        # 검증\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        correct_per_class = defaultdict(int)\n",
    "        total_per_class = defaultdict(int)\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "                \n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "                # F1 및 클래스별 accuracy 집계용\n",
    "                all_preds.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                for true_label, pred_label in zip(labels.cpu().numpy(), predictions.cpu().numpy()):\n",
    "                    total_per_class[true_label] += 1\n",
    "                    if true_label == pred_label:\n",
    "                        correct_per_class[true_label] += 1\n",
    "                \n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': f'{outputs.loss.item():.4f}',\n",
    "                    'acc': f'{(val_correct/val_total)*100:.2f}%'\n",
    "                })\n",
    "        \n",
    "        # 결과 계산\n",
    "        train_accuracy = (train_correct / train_total) * 100\n",
    "        val_accuracy = (val_correct / val_total) * 100\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "        print(f'\\nEpoch {epoch+1} 결과:')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_accuracy:.2f}%, F1 Score: {val_f1:.4f}')\n",
    "        \n",
    "        # 클래스별 Accuracy 출력\n",
    "        print(\"클래스별 Accuracy:\")\n",
    "        num_classes = len(set(all_labels))\n",
    "        for i in range(num_classes):\n",
    "            acc = correct_per_class[i] / total_per_class[i] if total_per_class[i] > 0 else 0.0\n",
    "            label_str = label_names[i] if label_names and i < len(label_names) else f\"Class {i}\"\n",
    "            print(f\"{label_str}: {acc:.2%} ({correct_per_class[i]}/{total_per_class[i]})\")\n",
    "        \n",
    "        # 최고 accuracy 모델 저장\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), '/kaggle/working/best_biobert_model.pt')\n",
    "            print(f'모델 저장됨 (Validation Accuracy: {val_accuracy:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T08:38:33.963873Z",
     "iopub.status.busy": "2025-05-20T08:38:33.963352Z",
     "iopub.status.idle": "2025-05-20T08:38:33.994087Z",
     "shell.execute_reply": "2025-05-20T08:38:33.993486Z",
     "shell.execute_reply.started": "2025-05-20T08:38:33.963849Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patiromer, an oral potassium (K(+)) binder, ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selenium (p = 0.0001, OR 0.788, 95% CI 0.703-0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Potassium may protect against MM, while Calciu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The guide outlines key dietary restrictions as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Culturally relevant substitutions and preparat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Patiromer, an oral potassium (K(+)) binder, ha...      0\n",
       "1  Selenium (p = 0.0001, OR 0.788, 95% CI 0.703-0...      1\n",
       "2  Potassium may protect against MM, while Calciu...      1\n",
       "3  The guide outlines key dietary restrictions as...      0\n",
       "4  Culturally relevant substitutions and preparat...      0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.read_csv('/kaggle/input/classify-pubmed/merged_data.csv')\n",
    "merged_df = merged_df[['text', 'label']]\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T08:38:34.380299Z",
     "iopub.status.busy": "2025-05-20T08:38:34.379591Z",
     "iopub.status.idle": "2025-05-20T08:51:07.284726Z",
     "shell.execute_reply": "2025-05-20T08:51:07.283986Z",
     "shell.execute_reply.started": "2025-05-20T08:38:34.380276Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10 [Train]: 100%|██████████| 68/68 [01:09<00:00,  1.02s/it, loss=1.2044, acc=38.71%]\n",
      "Epoch 1/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.85it/s, loss=1.1511, acc=57.72%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 결과:\n",
      "Training Loss: 1.4705, Accuracy: 38.71%\n",
      "Validation Loss: 1.2100, Accuracy: 57.72%, F1 Score: 0.5226\n",
      "클래스별 Accuracy:\n",
      "Class 0: 35.00% (14/40)\n",
      "Class 1: 87.04% (47/54)\n",
      "Class 2: 93.85% (61/65)\n",
      "Class 3: 41.38% (24/58)\n",
      "Class 4: 20.00% (11/55)\n",
      "모델 저장됨 (Validation Accuracy: 57.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 68/68 [01:09<00:00,  1.03s/it, loss=0.8092, acc=71.98%]\n",
      "Epoch 2/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.82it/s, loss=0.7155, acc=73.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 결과:\n",
      "Training Loss: 0.9007, Accuracy: 71.98%\n",
      "Validation Loss: 0.7908, Accuracy: 73.90%, F1 Score: 0.7277\n",
      "클래스별 Accuracy:\n",
      "Class 0: 60.00% (24/40)\n",
      "Class 1: 72.22% (39/54)\n",
      "Class 2: 86.15% (56/65)\n",
      "Class 3: 63.79% (37/58)\n",
      "Class 4: 81.82% (45/55)\n",
      "모델 저장됨 (Validation Accuracy: 73.90%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.3431, acc=86.27%]\n",
      "Epoch 3/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.83it/s, loss=0.5127, acc=76.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 결과:\n",
      "Training Loss: 0.4929, Accuracy: 86.27%\n",
      "Validation Loss: 0.6757, Accuracy: 76.47%, F1 Score: 0.7561\n",
      "클래스별 Accuracy:\n",
      "Class 0: 80.00% (32/40)\n",
      "Class 1: 87.04% (47/54)\n",
      "Class 2: 84.62% (55/65)\n",
      "Class 3: 53.45% (31/58)\n",
      "Class 4: 78.18% (43/55)\n",
      "모델 저장됨 (Validation Accuracy: 76.47%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0637, acc=94.10%]\n",
      "Epoch 4/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.81it/s, loss=0.4344, acc=80.51%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 결과:\n",
      "Training Loss: 0.2501, Accuracy: 94.10%\n",
      "Validation Loss: 0.6475, Accuracy: 80.51%, F1 Score: 0.7952\n",
      "클래스별 Accuracy:\n",
      "Class 0: 72.50% (29/40)\n",
      "Class 1: 87.04% (47/54)\n",
      "Class 2: 89.23% (58/65)\n",
      "Class 3: 63.79% (37/58)\n",
      "Class 4: 87.27% (48/55)\n",
      "모델 저장됨 (Validation Accuracy: 80.51%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0571, acc=97.42%]\n",
      "Epoch 5/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.82it/s, loss=0.6233, acc=81.99%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 결과:\n",
      "Training Loss: 0.1199, Accuracy: 97.42%\n",
      "Validation Loss: 0.6465, Accuracy: 81.99%, F1 Score: 0.8100\n",
      "클래스별 Accuracy:\n",
      "Class 0: 72.50% (29/40)\n",
      "Class 1: 92.59% (50/54)\n",
      "Class 2: 86.15% (56/65)\n",
      "Class 3: 65.52% (38/58)\n",
      "Class 4: 90.91% (50/55)\n",
      "모델 저장됨 (Validation Accuracy: 81.99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0424, acc=99.35%]\n",
      "Epoch 6/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.79it/s, loss=0.5711, acc=81.99%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 결과:\n",
      "Training Loss: 0.0497, Accuracy: 99.35%\n",
      "Validation Loss: 0.7050, Accuracy: 81.99%, F1 Score: 0.8115\n",
      "클래스별 Accuracy:\n",
      "Class 0: 80.00% (32/40)\n",
      "Class 1: 90.74% (49/54)\n",
      "Class 2: 84.62% (55/65)\n",
      "Class 3: 62.07% (36/58)\n",
      "Class 4: 92.73% (51/55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0169, acc=99.82%]\n",
      "Epoch 7/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.80it/s, loss=0.7734, acc=80.88%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 결과:\n",
      "Training Loss: 0.0286, Accuracy: 99.82%\n",
      "Validation Loss: 0.7580, Accuracy: 80.88%, F1 Score: 0.8012\n",
      "클래스별 Accuracy:\n",
      "Class 0: 70.00% (28/40)\n",
      "Class 1: 94.44% (51/54)\n",
      "Class 2: 86.15% (56/65)\n",
      "Class 3: 70.69% (41/58)\n",
      "Class 4: 80.00% (44/55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0236, acc=99.63%]\n",
      "Epoch 8/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.82it/s, loss=0.8827, acc=82.72%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 결과:\n",
      "Training Loss: 0.0228, Accuracy: 99.63%\n",
      "Validation Loss: 0.7812, Accuracy: 82.72%, F1 Score: 0.8163\n",
      "클래스별 Accuracy:\n",
      "Class 0: 62.50% (25/40)\n",
      "Class 1: 92.59% (50/54)\n",
      "Class 2: 83.08% (54/65)\n",
      "Class 3: 77.59% (45/58)\n",
      "Class 4: 92.73% (51/55)\n",
      "모델 저장됨 (Validation Accuracy: 82.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0105, acc=99.82%] \n",
      "Epoch 9/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.81it/s, loss=0.4008, acc=82.72%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 결과:\n",
      "Training Loss: 0.0141, Accuracy: 99.82%\n",
      "Validation Loss: 0.7828, Accuracy: 82.72%, F1 Score: 0.8201\n",
      "클래스별 Accuracy:\n",
      "Class 0: 72.50% (29/40)\n",
      "Class 1: 92.59% (50/54)\n",
      "Class 2: 87.69% (57/65)\n",
      "Class 3: 72.41% (42/58)\n",
      "Class 4: 85.45% (47/55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0056, acc=99.91%] \n",
      "Epoch 10/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.78it/s, loss=0.4568, acc=83.09%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 결과:\n",
      "Training Loss: 0.0102, Accuracy: 99.91%\n",
      "Validation Loss: 0.8066, Accuracy: 83.09%, F1 Score: 0.8219\n",
      "클래스별 Accuracy:\n",
      "Class 0: 80.00% (32/40)\n",
      "Class 1: 94.44% (51/54)\n",
      "Class 2: 86.15% (56/65)\n",
      "Class 3: 62.07% (36/58)\n",
      "Class 4: 92.73% (51/55)\n",
      "모델 저장됨 (Validation Accuracy: 83.09%)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 데이터 로드\n",
    "    # merged_df는 이전에 합친 데이터프레임\n",
    "    texts = merged_df['text'].values\n",
    "    labels = merged_df['label'].values\n",
    "    \n",
    "    # 데이터 분할\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 모델과 토크나이저 설정\n",
    "    num_labels = 5  # 레이블의 고유한 값 개수\n",
    "    model, tokenizer = setup_model(num_labels)\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "    \n",
    "    # 데이터로더 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    \n",
    "    # GPU 사용 가능 여부 확인\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 모델 학습\n",
    "    train_model(model, train_loader, val_loader, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T08:58:20.979734Z",
     "iopub.status.busy": "2025-05-20T08:58:20.979136Z",
     "iopub.status.idle": "2025-05-20T08:58:22.522328Z",
     "shell.execute_reply": "2025-05-20T08:58:22.521519Z",
     "shell.execute_reply.started": "2025-05-20T08:58:20.979711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 예: BioBERT 기반 5-class 분류 모델\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'dmis-lab/biobert-base-cased-v1.1',\n",
    "    num_labels=5  # 분류할 클래스 수에 맞게 수정\n",
    ")\n",
    "model.load_state_dict(torch.load('/kaggle/working/best_biobert_model.pt'))\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T09:03:51.432011Z",
     "iopub.status.busy": "2025-05-20T09:03:51.431729Z",
     "iopub.status.idle": "2025-05-20T09:03:52.319156Z",
     "shell.execute_reply": "2025-05-20T09:03:52.318409Z",
     "shell.execute_reply.started": "2025-05-20T09:03:51.431990Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 클래스: 3\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# 4. 예시 추론\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "inputs = tokenizer(\"Patients with malnutrition, alcoholism, inflammatory bowel disease, and malabsorption syndromes are at an increased risk of zinc deficiency.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    pred_class = torch.argmax(probs, dim=1)\n",
    "    print(\"예측 클래스:\", pred_class.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Classification 실행 후 분류 결과 저장 (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "model_path = \"./biobert_sentence_classifier\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "def classify_sentence(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        label = torch.argmax(probs, dim=1).item()\n",
    "    return label\n",
    "\n",
    "# 입력 CSV: 'sentences.csv' (컬럼: 'text')\n",
    "df = pd.read_csv(\"sentences.csv\")\n",
    "df['label'] = df['text'].apply(classify_sentence)\n",
    "df.to_csv(\"classified_sentences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER 정답 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(text):\n",
    "    return f\"\"\"\n",
    "다음 문장을 BIO 포맷으로 NER 태깅해줘. 가능한 태그는 다음과 같아:\n",
    "- INGREDIENT, SYMPTOM, DOSAGE, TARGET, SENSITIVE, GENDER, AGE_GROUP\n",
    "\n",
    "문장: \"{text}\"\n",
    "\n",
    "결과 형식 (JSON):\n",
    "{\n",
    "  \"tokens\": [...],\n",
    "  \"labels\": [...]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# ✅ 3. BioBERT로 NER fine-tuning 코드 (BIO 데이터 기반)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 데이터 로드 (BIO 포맷 JSONL)\n",
    "with open(\"ner_dataset.jsonl\", \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "label_list = [\"O\", \"B-INGREDIENT\", \"I-INGREDIENT\", \"B-DOSAGE\", \"I-DOSAGE\", \"B-SYMPTOM\", \"I-SYMPTOM\",\n",
    "              \"B-TARGET\", \"I-TARGET\", \"B-SENSITIVE\", \"I-SENSITIVE\", \"B-GENDER\", \"I-GENDER\", \"B-AGE_GROUP\", \"I-AGE_GROUP\"]\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# 전처리\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "tokens = [d['tokens'] for d in data]\n",
    "labels = [[label_to_id[tag] for tag in d['labels']] for d in data]\n",
    "\n",
    "encodings = tokenizer(tokens, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "# offset_mapping은 NER 학습에 불필요하므로 제거\n",
    "encodings.pop(\"offset_mapping\")\n",
    "\n",
    "dataset = NERDataset(encodings, labels)\n",
    "\n",
    "# 모델 및 학습 설정\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\", num_labels=len(label_list))\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./biobert_ner_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./biobert_ner_model\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7466277,
     "sourceId": 11880065,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp4pubmed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
