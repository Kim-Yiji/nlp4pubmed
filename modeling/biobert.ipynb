{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.82.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pandas in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (4.67.1)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yiji/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading openai-1.82.0-py3-none-any.whl (720 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.4/720.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jiter-0.10.0-cp311-cp311-macosx_11_0_arm64.whl (321 kB)\n",
      "Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.2/444.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-inspection, sniffio, python-dotenv, pydantic-core, jiter, h11, distro, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.10.0 openai-1.82.0 pydantic-2.11.5 pydantic-core-2.33.2 python-dotenv-1.1.0 sniffio-1.3.1 typing-inspection-0.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "%pip install python-dotenv openai pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 분류된 정답 데이터 기반 fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-20T07:05:11.140336Z",
     "iopub.status.busy": "2025-05-20T07:05:11.140051Z",
     "iopub.status.idle": "2025-05-20T07:05:11.146999Z",
     "shell.execute_reply": "2025-05-20T07:05:11.146227Z",
     "shell.execute_reply.started": "2025-05-20T07:05:11.140315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 데이터 준비\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T07:05:13.739771Z",
     "iopub.status.busy": "2025-05-20T07:05:13.739058Z",
     "iopub.status.idle": "2025-05-20T07:05:13.743741Z",
     "shell.execute_reply": "2025-05-20T07:05:13.742921Z",
     "shell.execute_reply.started": "2025-05-20T07:05:13.739743Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. 모델 설정\n",
    "def setup_model(num_labels):\n",
    "    # BioBERT 모델과 토크나이저 로드\n",
    "    model_name = \"dmis-lab/biobert-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T08:38:30.521748Z",
     "iopub.status.busy": "2025-05-20T08:38:30.521178Z",
     "iopub.status.idle": "2025-05-20T08:38:30.533562Z",
     "shell.execute_reply": "2025-05-20T08:38:30.532826Z",
     "shell.execute_reply.started": "2025-05-20T08:38:30.521727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=10, label_names=None):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.to(device)\n",
    "    best_val_accuracy = 0.0  # 최고 accuracy 추적\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 학습\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in train_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{(train_correct/train_total)*100:.2f}%'\n",
    "            })\n",
    "        \n",
    "        # 검증\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        correct_per_class = defaultdict(int)\n",
    "        total_per_class = defaultdict(int)\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "                \n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "                # F1 및 클래스별 accuracy 집계용\n",
    "                all_preds.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                for true_label, pred_label in zip(labels.cpu().numpy(), predictions.cpu().numpy()):\n",
    "                    total_per_class[true_label] += 1\n",
    "                    if true_label == pred_label:\n",
    "                        correct_per_class[true_label] += 1\n",
    "                \n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': f'{outputs.loss.item():.4f}',\n",
    "                    'acc': f'{(val_correct/val_total)*100:.2f}%'\n",
    "                })\n",
    "        \n",
    "        # 결과 계산\n",
    "        train_accuracy = (train_correct / train_total) * 100\n",
    "        val_accuracy = (val_correct / val_total) * 100\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "        print(f'\\nEpoch {epoch+1} 결과:')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_accuracy:.2f}%, F1 Score: {val_f1:.4f}')\n",
    "        \n",
    "        # 클래스별 Accuracy 출력\n",
    "        print(\"클래스별 Accuracy:\")\n",
    "        num_classes = len(set(all_labels))\n",
    "        for i in range(num_classes):\n",
    "            acc = correct_per_class[i] / total_per_class[i] if total_per_class[i] > 0 else 0.0\n",
    "            label_str = label_names[i] if label_names and i < len(label_names) else f\"Class {i}\"\n",
    "            print(f\"{label_str}: {acc:.2%} ({correct_per_class[i]}/{total_per_class[i]})\")\n",
    "        \n",
    "        # 최고 accuracy 모델 저장\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), '/kaggle/working/best_biobert_model.pt')\n",
    "            print(f'모델 저장됨 (Validation Accuracy: {val_accuracy:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T08:38:33.963873Z",
     "iopub.status.busy": "2025-05-20T08:38:33.963352Z",
     "iopub.status.idle": "2025-05-20T08:38:33.994087Z",
     "shell.execute_reply": "2025-05-20T08:38:33.993486Z",
     "shell.execute_reply.started": "2025-05-20T08:38:33.963849Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patiromer, an oral potassium (K(+)) binder, ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selenium (p = 0.0001, OR 0.788, 95% CI 0.703-0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Potassium may protect against MM, while Calciu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The guide outlines key dietary restrictions as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Culturally relevant substitutions and preparat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Patiromer, an oral potassium (K(+)) binder, ha...      0\n",
       "1  Selenium (p = 0.0001, OR 0.788, 95% CI 0.703-0...      1\n",
       "2  Potassium may protect against MM, while Calciu...      1\n",
       "3  The guide outlines key dietary restrictions as...      0\n",
       "4  Culturally relevant substitutions and preparat...      0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.read_csv('/kaggle/input/classify-pubmed/merged_data.csv')\n",
    "merged_df = merged_df[['text', 'label']]\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T08:38:34.380299Z",
     "iopub.status.busy": "2025-05-20T08:38:34.379591Z",
     "iopub.status.idle": "2025-05-20T08:51:07.284726Z",
     "shell.execute_reply": "2025-05-20T08:51:07.283986Z",
     "shell.execute_reply.started": "2025-05-20T08:38:34.380276Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10 [Train]: 100%|██████████| 68/68 [01:09<00:00,  1.02s/it, loss=1.2044, acc=38.71%]\n",
      "Epoch 1/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.85it/s, loss=1.1511, acc=57.72%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 결과:\n",
      "Training Loss: 1.4705, Accuracy: 38.71%\n",
      "Validation Loss: 1.2100, Accuracy: 57.72%, F1 Score: 0.5226\n",
      "클래스별 Accuracy:\n",
      "Class 0: 35.00% (14/40)\n",
      "Class 1: 87.04% (47/54)\n",
      "Class 2: 93.85% (61/65)\n",
      "Class 3: 41.38% (24/58)\n",
      "Class 4: 20.00% (11/55)\n",
      "모델 저장됨 (Validation Accuracy: 57.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 68/68 [01:09<00:00,  1.03s/it, loss=0.8092, acc=71.98%]\n",
      "Epoch 2/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.82it/s, loss=0.7155, acc=73.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 결과:\n",
      "Training Loss: 0.9007, Accuracy: 71.98%\n",
      "Validation Loss: 0.7908, Accuracy: 73.90%, F1 Score: 0.7277\n",
      "클래스별 Accuracy:\n",
      "Class 0: 60.00% (24/40)\n",
      "Class 1: 72.22% (39/54)\n",
      "Class 2: 86.15% (56/65)\n",
      "Class 3: 63.79% (37/58)\n",
      "Class 4: 81.82% (45/55)\n",
      "모델 저장됨 (Validation Accuracy: 73.90%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.3431, acc=86.27%]\n",
      "Epoch 3/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.83it/s, loss=0.5127, acc=76.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 결과:\n",
      "Training Loss: 0.4929, Accuracy: 86.27%\n",
      "Validation Loss: 0.6757, Accuracy: 76.47%, F1 Score: 0.7561\n",
      "클래스별 Accuracy:\n",
      "Class 0: 80.00% (32/40)\n",
      "Class 1: 87.04% (47/54)\n",
      "Class 2: 84.62% (55/65)\n",
      "Class 3: 53.45% (31/58)\n",
      "Class 4: 78.18% (43/55)\n",
      "모델 저장됨 (Validation Accuracy: 76.47%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0637, acc=94.10%]\n",
      "Epoch 4/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.81it/s, loss=0.4344, acc=80.51%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 결과:\n",
      "Training Loss: 0.2501, Accuracy: 94.10%\n",
      "Validation Loss: 0.6475, Accuracy: 80.51%, F1 Score: 0.7952\n",
      "클래스별 Accuracy:\n",
      "Class 0: 72.50% (29/40)\n",
      "Class 1: 87.04% (47/54)\n",
      "Class 2: 89.23% (58/65)\n",
      "Class 3: 63.79% (37/58)\n",
      "Class 4: 87.27% (48/55)\n",
      "모델 저장됨 (Validation Accuracy: 80.51%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0571, acc=97.42%]\n",
      "Epoch 5/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.82it/s, loss=0.6233, acc=81.99%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 결과:\n",
      "Training Loss: 0.1199, Accuracy: 97.42%\n",
      "Validation Loss: 0.6465, Accuracy: 81.99%, F1 Score: 0.8100\n",
      "클래스별 Accuracy:\n",
      "Class 0: 72.50% (29/40)\n",
      "Class 1: 92.59% (50/54)\n",
      "Class 2: 86.15% (56/65)\n",
      "Class 3: 65.52% (38/58)\n",
      "Class 4: 90.91% (50/55)\n",
      "모델 저장됨 (Validation Accuracy: 81.99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0424, acc=99.35%]\n",
      "Epoch 6/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.79it/s, loss=0.5711, acc=81.99%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 결과:\n",
      "Training Loss: 0.0497, Accuracy: 99.35%\n",
      "Validation Loss: 0.7050, Accuracy: 81.99%, F1 Score: 0.8115\n",
      "클래스별 Accuracy:\n",
      "Class 0: 80.00% (32/40)\n",
      "Class 1: 90.74% (49/54)\n",
      "Class 2: 84.62% (55/65)\n",
      "Class 3: 62.07% (36/58)\n",
      "Class 4: 92.73% (51/55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0169, acc=99.82%]\n",
      "Epoch 7/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.80it/s, loss=0.7734, acc=80.88%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 결과:\n",
      "Training Loss: 0.0286, Accuracy: 99.82%\n",
      "Validation Loss: 0.7580, Accuracy: 80.88%, F1 Score: 0.8012\n",
      "클래스별 Accuracy:\n",
      "Class 0: 70.00% (28/40)\n",
      "Class 1: 94.44% (51/54)\n",
      "Class 2: 86.15% (56/65)\n",
      "Class 3: 70.69% (41/58)\n",
      "Class 4: 80.00% (44/55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0236, acc=99.63%]\n",
      "Epoch 8/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.82it/s, loss=0.8827, acc=82.72%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 결과:\n",
      "Training Loss: 0.0228, Accuracy: 99.63%\n",
      "Validation Loss: 0.7812, Accuracy: 82.72%, F1 Score: 0.8163\n",
      "클래스별 Accuracy:\n",
      "Class 0: 62.50% (25/40)\n",
      "Class 1: 92.59% (50/54)\n",
      "Class 2: 83.08% (54/65)\n",
      "Class 3: 77.59% (45/58)\n",
      "Class 4: 92.73% (51/55)\n",
      "모델 저장됨 (Validation Accuracy: 82.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0105, acc=99.82%] \n",
      "Epoch 9/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.81it/s, loss=0.4008, acc=82.72%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 결과:\n",
      "Training Loss: 0.0141, Accuracy: 99.82%\n",
      "Validation Loss: 0.7828, Accuracy: 82.72%, F1 Score: 0.8201\n",
      "클래스별 Accuracy:\n",
      "Class 0: 72.50% (29/40)\n",
      "Class 1: 92.59% (50/54)\n",
      "Class 2: 87.69% (57/65)\n",
      "Class 3: 72.41% (42/58)\n",
      "Class 4: 85.45% (47/55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0056, acc=99.91%] \n",
      "Epoch 10/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.78it/s, loss=0.4568, acc=83.09%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 결과:\n",
      "Training Loss: 0.0102, Accuracy: 99.91%\n",
      "Validation Loss: 0.8066, Accuracy: 83.09%, F1 Score: 0.8219\n",
      "클래스별 Accuracy:\n",
      "Class 0: 80.00% (32/40)\n",
      "Class 1: 94.44% (51/54)\n",
      "Class 2: 86.15% (56/65)\n",
      "Class 3: 62.07% (36/58)\n",
      "Class 4: 92.73% (51/55)\n",
      "모델 저장됨 (Validation Accuracy: 83.09%)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 데이터 로드\n",
    "    # merged_df는 이전에 합친 데이터프레임\n",
    "    texts = merged_df['text'].values\n",
    "    labels = merged_df['label'].values\n",
    "    \n",
    "    # 데이터 분할\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 모델과 토크나이저 설정\n",
    "    num_labels = 5  # 레이블의 고유한 값 개수\n",
    "    model, tokenizer = setup_model(num_labels)\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "    \n",
    "    # 데이터로더 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    \n",
    "    # GPU 사용 가능 여부 확인\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 모델 학습\n",
    "    train_model(model, train_loader, val_loader, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T08:58:20.979734Z",
     "iopub.status.busy": "2025-05-20T08:58:20.979136Z",
     "iopub.status.idle": "2025-05-20T08:58:22.522328Z",
     "shell.execute_reply": "2025-05-20T08:58:22.521519Z",
     "shell.execute_reply.started": "2025-05-20T08:58:20.979711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 예: BioBERT 기반 5-class 분류 모델\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'dmis-lab/biobert-base-cased-v1.1',\n",
    "    num_labels=5  # 분류할 클래스 수에 맞게 수정\n",
    ")\n",
    "model.load_state_dict(torch.load('/kaggle/working/best_biobert_model.pt'))\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T09:03:51.432011Z",
     "iopub.status.busy": "2025-05-20T09:03:51.431729Z",
     "iopub.status.idle": "2025-05-20T09:03:52.319156Z",
     "shell.execute_reply": "2025-05-20T09:03:52.318409Z",
     "shell.execute_reply.started": "2025-05-20T09:03:51.431990Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 클래스: 3\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# 4. 예시 추론\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "inputs = tokenizer(\"Patients with malnutrition, alcoholism, inflammatory bowel disease, and malabsorption syndromes are at an increased risk of zinc deficiency.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    pred_class = torch.argmax(probs, dim=1)\n",
    "    print(\"예측 클래스:\", pred_class.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!Sentence Classification 용 데이터베이스 구축 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Classification 실행 후 분류 결과 저장 (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "model_path = \"./best_biobert_model.pt\"  # 저장된 모델 경로\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "def classify_sentence(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        label = torch.argmax(probs, dim=1).item()\n",
    "    return label\n",
    "\n",
    "# 입력 CSV: 'sentences.csv' (컬럼: 'text')\n",
    "df = pd.read_csv(\"sentences.csv\")\n",
    "df['label'] = df['text'].apply(classify_sentence)\n",
    "df.to_csv(\"classified_sentences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER 정답 데이터 생성\n",
    "# !!!! id 부여 해서 관리해야함\n",
    "- sentence classification 정답 데이터 기반 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2.0.0\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.4\n",
      "    Uninstalling numpy-2.2.4:\n",
      "      Successfully uninstalled numpy-2.2.4\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy<2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.61.1\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: \n",
      "Location: /opt/anaconda3/lib/python3.12/site-packages\n",
      "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER 처리 중: 100%|██████████| 1133/1133 [55:21<00:00,  2.93s/it] \n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 환경 변수에서 API 키 로드\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "INPUT_CSV_PATH = \"Database/NER/sentence_classification_answer_not_100.csv\"\n",
    "OUTPUT_JSONL_PATH = \"Database/NER/gpt4_ner_results_not_100.jsonl\"\n",
    "MODEL_NAME = \"gpt-4-turbo\"\n",
    "\n",
    "# 프롬프트 생성 함수\n",
    "def create_prompt(text):\n",
    "    return f\"\"\"\n",
    "다음 문장에서 영양소 관련 엔티티를 추출해줘. 각 엔티티는 다음과 같은 유형 중 하나야:\n",
    "\n",
    "- INGREDIENT: 구체적인 영양소 또는 화학 성분 이름 (예: \"Vitamin A\", \"Potassium\", \"Iron\"). 일반적인 물질이나 작용(ex. \"fluid\", \"energy\")은 제외.\n",
    "- SYMPTOM: 명확히 정의된 질병명이나 의학적 증상만 포함해줘. \"risk\", \"likelihood\", \"association\" 같은 표현은 제외하고, \"MM risk\"와 같은 경우엔 \"MM\"만 추출해줘.\n",
    "- DOSAGE: 수치 + 단위 조합으로 된 복용량만 포함 (예: \"5000 IU/day\", \"10mg of iron\").\n",
    "- SENSITIVE_CONDITION: 임신, 수유, 약물복용, 노인, 어린이 등의 민감 조건.\n",
    "- PERSONAL_INFO: 나이, 성별, 키, 몸무게 등 인구통계적 정보.\n",
    "\n",
    "반드시 아래와 같은 JSON 형식으로만 응답해줘:\n",
    "\n",
    "{{\n",
    "  \"text\": \"<원본 문장>\",\n",
    "  \"entities\": {{\n",
    "    \"INGREDIENT\": [...],\n",
    "    \"SYMPTOM\": [...],\n",
    "    \"DOSAGE\": [...],\n",
    "    \"SENSITIVE_CONDITION\": [...],\n",
    "    \"PERSONAL_INFO\": [...]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "다음 문장:\n",
    "\\\"{text}\\\"\n",
    "\"\"\".strip()\n",
    "\n",
    "# GPT 호출 함수\n",
    "def extract_entities_with_gpt(text, label, model=MODEL_NAME):\n",
    "    # ✅ label이 0이면 GPT 호출 자체를 생략\n",
    "    if label == 0:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"너는 pubmed 데이터를 기반으로 한 의학 논문에서 정보를 구조화해주는 전문가야.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": create_prompt(text)\n",
    "                }\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        return json.loads(content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\"text\": text, \"error\": f\"JSON 파싱 실패: {str(e)}\", \"raw\": content}\n",
    "    except Exception as e:\n",
    "        return {\"text\": text, \"error\": str(e)}\n",
    "\n",
    "# CSV 불러오기\n",
    "df = pd.read_csv(INPUT_CSV_PATH)\n",
    "if \"text\" not in df.columns or \"label\" not in df.columns:\n",
    "    raise ValueError(\"CSV에 'text' 또는 'label' 컬럼이 없습니다.\")\n",
    "\n",
    "# 저장\n",
    "with open(OUTPUT_JSONL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"NER 처리 중\"):\n",
    "        text = row[\"text\"]\n",
    "        label = row[\"label\"]\n",
    "\n",
    "        result = extract_entities_with_gpt(text, label)\n",
    "        if result is not None:\n",
    "            f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "            f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단일 문장 테스팅용 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT 엔티티 추출 중: 100%|██████████| 3/3 [00:08<00:00,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"Selenium (p = 0.0001, OR 0.788, 95% CI 0.703-0.883) and Potassium (p = 0.045, OR 0.463, 95% CI 0.219-0.982) were significantly negatively associated with MM risk, suggesting a protective effect.\",\n",
      "    \"entities\": {\n",
      "      \"INGREDIENT\": [\n",
      "        \"Selenium\",\n",
      "        \"Potassium\"\n",
      "      ],\n",
      "      \"SYMPTOM\": [\n",
      "        \"MM risk\"\n",
      "      ],\n",
      "      \"DOSAGE\": [],\n",
      "      \"SENSITIVE_CONDITION\": [],\n",
      "      \"PERSONAL_INFO\": []\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"The guide outlines key dietary restrictions associated with dialysis-specifically for phosphorus, potassium, sodium, and fluid intake-and presents alternatives using familiar Filipino foods.\",\n",
      "    \"entities\": {\n",
      "      \"INGREDIENT\": [\n",
      "        \"phosphorus\",\n",
      "        \"potassium\",\n",
      "        \"sodium\"\n",
      "      ],\n",
      "      \"SYMPTOM\": [\n",
      "        \"dialysis\"\n",
      "      ],\n",
      "      \"SENSITIVE_CONDITION\": []\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"This panting causes respiratory hypocapnia, which increases the renal excretion of buffer molecules including sodium, potassium, and bicarbonate.\",\n",
      "    \"entities\": {\n",
      "      \"INGREDIENT\": [\n",
      "        \"Sodium\",\n",
      "        \"Potassium\",\n",
      "        \"Bicarbonate\"\n",
      "      ],\n",
      "      \"SYMPTOM\": [\n",
      "        \"Respiratory hypocapnia\"\n",
      "      ],\n",
      "      \"DOSAGE\": [],\n",
      "      \"SENSITIVE_CONDITION\": [],\n",
      "      \"PERSONAL_INFO\": []\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 환경 변수에서 API 키 로드\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# 👉 여러 문장 테스트\n",
    "test_sentences = [\n",
    "    \"Selenium (p = 0.0001, OR 0.788, 95% CI 0.703-0.883) and Potassium (p = 0.045, OR 0.463, 95% CI 0.219-0.982) were significantly negatively associated with MM risk, suggesting a protective effect.\",\n",
    "    \"The guide outlines key dietary restrictions associated with dialysis-specifically for phosphorus, potassium, sodium, and fluid intake-and presents alternatives using familiar Filipino foods.\",\n",
    "    \"This panting causes respiratory hypocapnia, which increases the renal excretion of buffer molecules including sodium, potassium, and bicarbonate.\"\n",
    "]\n",
    "\n",
    "def create_prompt(text):\n",
    "    return f\"\"\"\n",
    "다음 문장에서 영양소 관련 엔티티를 추출해줘. 각 엔티티는 다음과 같은 유형 중 하나야:\n",
    "\n",
    "- INGREDIENT: 구체적인 영양소 또는 화학 성분 이름 (예: \"Vitamin A\", \"Potassium\", \"Iron\"). 일반적인 물질이나 작용(ex. \"fluid\", \"energy\")은 제외.\n",
    "- SYMPTOM: 명확히 정의된 질병명이나 의학적 증상만 포함해줘. \"risk\", \"likelihood\", \"association\" 같은 표현은 제외하고, \"MM risk\"와 같은 경우엔 \"MM\"만 추출해줘.\n",
    "- DOSAGE: 수치 + 단위 조합으로 된 복용량만 포함 (예: \"5000 IU/day\", \"10mg of iron\").\n",
    "- SENSITIVE_CONDITION: 임신, 수유, 약물복용, 노인, 어린이 등의 민감 조건.\n",
    "- PERSONAL_INFO: 나이, 성별, 키, 몸무게 등 인구통계적 정보.\n",
    "\n",
    "반드시 아래와 같은 JSON 형식으로만 응답해줘:\n",
    "\n",
    "{{\n",
    "  \"text\": \"<원본 문장>\",\n",
    "  \"entities\": {{\n",
    "    \"INGREDIENT\": [...],\n",
    "    \"SYMPTOM\": [...],\n",
    "    \"DOSAGE\": [...],\n",
    "    \"SENSITIVE_CONDITION\": [...],\n",
    "    \"PERSONAL_INFO\": [...]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "다음 문장:\n",
    "\\\"{text}\\\"\n",
    "\"\"\"\n",
    "\n",
    "# GPT 호출 함수\n",
    "def extract_entities_with_gpt(text, model=\"gpt-3.5-turbo-1106\"): # gpt-4-turbo-1106 모델 사용\n",
    "    \"\"\"\n",
    "    GPT API를 사용하여 문장에서 엔티티를 추출하는 함수.\n",
    "    예외 처리 및 JSON 파싱 안정성 추가.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"너는 pubmed 데이터를 기반으로 한 의학 논문에서 정보를 구조화해주는 전문가야.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": create_prompt(text)\n",
    "                }\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content.strip()\n",
    "\n",
    "        # 응답이 JSON이 아닐 수 있으므로 확인\n",
    "        return json.loads(content)\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\"text\": text, \"error\": f\"JSON 파싱 실패: {str(e)}\", \"raw\": content}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"text\": text, \"error\": str(e)}\n",
    "\n",
    "# 실행 (with tqdm)\n",
    "results = []\n",
    "for sentence in tqdm(test_sentences, desc=\"GPT 엔티티 추출 중\"):\n",
    "    result = extract_entities_with_gpt(sentence)\n",
    "    results.append(result)\n",
    "\n",
    "# 결과 확인\n",
    "print(json.dumps(results, indent=2, ensure_ascii=False))\n",
    "with open(\"gpt4_ner_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIO 포맷으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ner_structured_results.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 5. 입력 JSONL 읽고 변환\u001b[39;00m\n\u001b[32m     32\u001b[39m bio_data = []\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     35\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/YONSEI/텍스트정보처리론/nlp4pubmed/nlp4pubmed/lib/python3.11/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'ner_structured_results.jsonl'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. BioBERT 토크나이저 로딩\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "# 2. 파일 경로 설정\n",
    "input_path = \"Database/NER/gpt4_ner_results.jsonl\"\n",
    "output_path = \"Database/NER/ner_bio_format_results.jsonl\"\n",
    "\n",
    "# 3. 사용할 엔티티 태그 종류\n",
    "entity_labels = [\"INGREDIENT\", \"SYMPTOM\", \"DOSAGE\", \"SENSITIVE_CONDITION\", \"PERSONAL_INFO\"]\n",
    "\n",
    "# 4. BIO 태깅 함수\n",
    "def convert_to_bio(text, entities):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    labels = ['O'] * len(tokens)\n",
    "\n",
    "    for ent_type in entity_labels:\n",
    "        for phrase in entities.get(ent_type, []):\n",
    "            phrase_tokens = tokenizer.tokenize(phrase)\n",
    "            for i in range(len(tokens) - len(phrase_tokens) + 1):\n",
    "                if tokens[i:i + len(phrase_tokens)] == phrase_tokens:\n",
    "                    labels[i] = f'B-{ent_type}'\n",
    "                    for j in range(1, len(phrase_tokens)):\n",
    "                        labels[i + j] = f'I-{ent_type}'\n",
    "                    break  # 하나만 처리하고 끝\n",
    "\n",
    "    return {\"tokens\": tokens, \"labels\": labels}\n",
    "\n",
    "# 5. 입력 JSONL 읽고 변환\n",
    "bio_data = []\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            item = json.loads(line)\n",
    "            if \"text\" in item and \"entities\" in item:\n",
    "                bio_entry = convert_to_bio(item[\"text\"], item[\"entities\"])\n",
    "                bio_data.append(bio_entry)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing line: {e}\")\n",
    "\n",
    "# 6. BIO JSONL 저장 (한 줄씩 JSON 객체로)\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in bio_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "print(f\"✅ BIO 포맷 변환 완료! 저장 경로: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 테스팅용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BIO 변환 완료! 저장 위치: ner_bio_format.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# BioBERT tokenizer 로딩\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "# 입력 및 출력 파일 경로\n",
    "input_path = \"gpt4_ner_results.json\"\n",
    "output_path = \"ner_bio_format.json\"\n",
    "\n",
    "# 엔티티 타입 리스트\n",
    "entity_labels = [\"INGREDIENT\", \"SYMPTOM\", \"DOSAGE\", \"SENSITIVE_CONDITION\", \"PERSONAL_INFO\"]\n",
    "\n",
    "# BIO 변환 함수\n",
    "def convert_to_bio(text, entities):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    labels = ['O'] * len(tokens)\n",
    "\n",
    "    for ent_type in entity_labels:\n",
    "        for phrase in entities.get(ent_type, []):\n",
    "            phrase_tokens = tokenizer.tokenize(phrase)\n",
    "            for i in range(len(tokens) - len(phrase_tokens) + 1):\n",
    "                if tokens[i:i + len(phrase_tokens)] == phrase_tokens:\n",
    "                    labels[i] = f\"B-{ent_type}\"\n",
    "                    for j in range(1, len(phrase_tokens)):\n",
    "                        labels[i + j] = f\"I-{ent_type}\"\n",
    "                    break  # 중복 방지\n",
    "    return {\"tokens\": tokens, \"labels\": labels}\n",
    "\n",
    "# 파일 로딩 및 변환 실행\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "bio_data = []\n",
    "for item in data:\n",
    "    if \"text\" in item and \"entities\" in item:\n",
    "        bio_entry = convert_to_bio(item[\"text\"], item[\"entities\"])\n",
    "        bio_data.append(bio_entry)\n",
    "\n",
    "# 저장\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(bio_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ BIO 변환 완료! 저장 위치: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 데이터 로드 (BIO 포맷 JSONL)\n",
    "with open(\"ner_dataset.jsonl\", \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "label_list = [\"O\", \"B-INGREDIENT\", \"I-INGREDIENT\", \"B-DOSAGE\", \"I-DOSAGE\", \"B-SYMPTOM\", \"I-SYMPTOM\",\n",
    "              \"B-TARGET\", \"I-TARGET\", \"B-SENSITIVE\", \"I-SENSITIVE\", \"B-GENDER\", \"I-GENDER\", \"B-AGE_GROUP\", \"I-AGE_GROUP\"]\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# 전처리\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "tokens = [d['tokens'] for d in data]\n",
    "labels = [[label_to_id[tag] for tag in d['labels']] for d in data]\n",
    "\n",
    "encodings = tokenizer(tokens, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "# offset_mapping은 NER 학습에 불필요하므로 제거\n",
    "encodings.pop(\"offset_mapping\")\n",
    "\n",
    "dataset = NERDataset(encodings, labels)\n",
    "\n",
    "# 모델 및 학습 설정\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\", num_labels=len(label_list))\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./biobert_ner_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./biobert_ner_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rag용 메타데이터 형식으로 재변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def merge_tokens(tokens):\n",
    "    text = \"\"\n",
    "    for tok in tokens:\n",
    "        if tok.startswith(\"##\"):\n",
    "            text += tok[2:]\n",
    "        elif text:\n",
    "            text += \" \" + tok\n",
    "        else:\n",
    "            text = tok\n",
    "    return text.strip()\n",
    "\n",
    "def recover_entities(tokens, labels):\n",
    "    entities = {\n",
    "        \"INGREDIENT\": [],\n",
    "        \"SYMPTOM\": [],\n",
    "        \"DOSAGE\": [],\n",
    "        \"SENSITIVE_CONDITION\": [],\n",
    "        \"PERSONAL_INFO\": []\n",
    "    }\n",
    "    current_tokens = []\n",
    "    current_type = None\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label == \"O\":\n",
    "            if current_tokens:\n",
    "                entities[current_type].append(merge_tokens(current_tokens))\n",
    "                current_tokens = []\n",
    "                current_type = None\n",
    "        elif label.startswith(\"B-\"):\n",
    "            if current_tokens:\n",
    "                entities[current_type].append(merge_tokens(current_tokens))\n",
    "            current_type = label[2:]\n",
    "            current_tokens = [token]\n",
    "        elif label.startswith(\"I-\") and current_type == label[2:]:\n",
    "            current_tokens.append(token)\n",
    "        else:\n",
    "            if current_tokens:\n",
    "                entities[current_type].append(merge_tokens(current_tokens))\n",
    "            current_tokens = []\n",
    "            current_type = None\n",
    "\n",
    "    if current_tokens:\n",
    "        entities[current_type].append(merge_tokens(current_tokens))\n",
    "\n",
    "    return entities\n",
    "\n",
    "def convert_bio_json_to_rag_jsonl(input_path, output_path=\"rag_converted.jsonl\"):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        bio_data = json.load(f)\n",
    "\n",
    "    rag_data = []\n",
    "    for item in bio_data:\n",
    "        tokens = item[\"tokens\"]\n",
    "        labels = item[\"labels\"]\n",
    "        text = merge_tokens(tokens)\n",
    "        meta = recover_entities(tokens, labels)\n",
    "        rag_data.append({\n",
    "            \"text\": text,\n",
    "            \"meta\": meta\n",
    "        })\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in rag_data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ 변환 완료: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 변환 완료: rag_converted.jsonl\n"
     ]
    }
   ],
   "source": [
    "convert_bio_json_to_rag_jsonl(\"ner_bio_format.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7466277,
     "sourceId": 11880065,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
