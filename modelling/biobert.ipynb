{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11880065,"sourceType":"datasetVersion","datasetId":7466277}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\n\n# 1. 데이터 준비\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T07:05:11.140051Z","iopub.execute_input":"2025-05-20T07:05:11.140336Z","iopub.status.idle":"2025-05-20T07:05:11.146999Z","shell.execute_reply.started":"2025-05-20T07:05:11.140315Z","shell.execute_reply":"2025-05-20T07:05:11.146227Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# 2. 모델 설정\ndef setup_model(num_labels):\n    # BioBERT 모델과 토크나이저 로드\n    model_name = \"dmis-lab/biobert-v1.1\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=num_labels\n    )\n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T07:05:13.739058Z","iopub.execute_input":"2025-05-20T07:05:13.739771Z","iopub.status.idle":"2025-05-20T07:05:13.743741Z","shell.execute_reply.started":"2025-05-20T07:05:13.739743Z","shell.execute_reply":"2025-05-20T07:05:13.742921Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.metrics import f1_score\nfrom collections import defaultdict\nimport numpy as np\n\ndef train_model(model, train_loader, val_loader, device, num_epochs=10, label_names=None):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    model.to(device)\n    best_val_accuracy = 0.0  # 최고 accuracy 추적\n    \n    for epoch in range(num_epochs):\n        # 학습\n        model.train()\n        train_loss = 0\n        train_correct = 0\n        train_total = 0\n        \n        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n        for batch in train_pbar:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            predictions = torch.argmax(outputs.logits, dim=1)\n            train_correct += (predictions == labels).sum().item()\n            train_total += labels.size(0)\n            \n            train_pbar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc': f'{(train_correct/train_total)*100:.2f}%'\n            })\n        \n        # 검증\n        model.eval()\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n        all_preds = []\n        all_labels = []\n\n        correct_per_class = defaultdict(int)\n        total_per_class = defaultdict(int)\n        \n        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n        with torch.no_grad():\n            for batch in val_pbar:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                \n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                val_loss += outputs.loss.item()\n                \n                predictions = torch.argmax(outputs.logits, dim=1)\n                val_correct += (predictions == labels).sum().item()\n                val_total += labels.size(0)\n\n                # F1 및 클래스별 accuracy 집계용\n                all_preds.extend(predictions.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n                for true_label, pred_label in zip(labels.cpu().numpy(), predictions.cpu().numpy()):\n                    total_per_class[true_label] += 1\n                    if true_label == pred_label:\n                        correct_per_class[true_label] += 1\n                \n                val_pbar.set_postfix({\n                    'loss': f'{outputs.loss.item():.4f}',\n                    'acc': f'{(val_correct/val_total)*100:.2f}%'\n                })\n        \n        # 결과 계산\n        train_accuracy = (train_correct / train_total) * 100\n        val_accuracy = (val_correct / val_total) * 100\n        val_f1 = f1_score(all_labels, all_preds, average='macro')\n\n        print(f'\\nEpoch {epoch+1} 결과:')\n        print(f'Training Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%')\n        print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_accuracy:.2f}%, F1 Score: {val_f1:.4f}')\n        \n        # 클래스별 Accuracy 출력\n        print(\"클래스별 Accuracy:\")\n        num_classes = len(set(all_labels))\n        for i in range(num_classes):\n            acc = correct_per_class[i] / total_per_class[i] if total_per_class[i] > 0 else 0.0\n            label_str = label_names[i] if label_names and i < len(label_names) else f\"Class {i}\"\n            print(f\"{label_str}: {acc:.2%} ({correct_per_class[i]}/{total_per_class[i]})\")\n        \n        # 최고 accuracy 모델 저장\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            torch.save(model.state_dict(), '/kaggle/working/best_biobert_model.pt')\n            print(f'모델 저장됨 (Validation Accuracy: {val_accuracy:.2f}%)')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:38:30.521178Z","iopub.execute_input":"2025-05-20T08:38:30.521748Z","iopub.status.idle":"2025-05-20T08:38:30.533562Z","shell.execute_reply.started":"2025-05-20T08:38:30.521727Z","shell.execute_reply":"2025-05-20T08:38:30.532826Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"merged_df = pd.read_csv('/kaggle/input/classify-pubmed/merged_data.csv')\nmerged_df = merged_df[['text', 'label']]\nmerged_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:38:33.963352Z","iopub.execute_input":"2025-05-20T08:38:33.963873Z","iopub.status.idle":"2025-05-20T08:38:33.994087Z","shell.execute_reply.started":"2025-05-20T08:38:33.963849Z","shell.execute_reply":"2025-05-20T08:38:33.993486Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0  Patiromer, an oral potassium (K(+)) binder, ha...      0\n1  Selenium (p = 0.0001, OR 0.788, 95% CI 0.703-0...      1\n2  Potassium may protect against MM, while Calciu...      1\n3  The guide outlines key dietary restrictions as...      0\n4  Culturally relevant substitutions and preparat...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Patiromer, an oral potassium (K(+)) binder, ha...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Selenium (p = 0.0001, OR 0.788, 95% CI 0.703-0...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Potassium may protect against MM, while Calciu...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The guide outlines key dietary restrictions as...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Culturally relevant substitutions and preparat...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"def main():\n    # 데이터 로드\n    # merged_df는 이전에 합친 데이터프레임\n    texts = merged_df['text'].values\n    labels = merged_df['label'].values\n    \n    # 데이터 분할\n    train_texts, val_texts, train_labels, val_labels = train_test_split(\n        texts, labels, test_size=0.2, random_state=42\n    )\n    \n    # 모델과 토크나이저 설정\n    num_labels = 5  # 레이블의 고유한 값 개수\n    model, tokenizer = setup_model(num_labels)\n    \n    # 데이터셋 생성\n    train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n    val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n    \n    # 데이터로더 생성\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=16)\n    \n    # GPU 사용 가능 여부 확인\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # 모델 학습\n    train_model(model, train_loader, val_loader, device)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:38:34.379591Z","iopub.execute_input":"2025-05-20T08:38:34.380299Z","iopub.status.idle":"2025-05-20T08:51:07.284726Z","shell.execute_reply.started":"2025-05-20T08:38:34.380276Z","shell.execute_reply":"2025-05-20T08:51:07.283986Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/10 [Train]: 100%|██████████| 68/68 [01:09<00:00,  1.02s/it, loss=1.2044, acc=38.71%]\nEpoch 1/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.85it/s, loss=1.1511, acc=57.72%]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 결과:\nTraining Loss: 1.4705, Accuracy: 38.71%\nValidation Loss: 1.2100, Accuracy: 57.72%, F1 Score: 0.5226\n클래스별 Accuracy:\nClass 0: 35.00% (14/40)\nClass 1: 87.04% (47/54)\nClass 2: 93.85% (61/65)\nClass 3: 41.38% (24/58)\nClass 4: 20.00% (11/55)\n모델 저장됨 (Validation Accuracy: 57.72%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10 [Train]: 100%|██████████| 68/68 [01:09<00:00,  1.03s/it, loss=0.8092, acc=71.98%]\nEpoch 2/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.82it/s, loss=0.7155, acc=73.90%]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 결과:\nTraining Loss: 0.9007, Accuracy: 71.98%\nValidation Loss: 0.7908, Accuracy: 73.90%, F1 Score: 0.7277\n클래스별 Accuracy:\nClass 0: 60.00% (24/40)\nClass 1: 72.22% (39/54)\nClass 2: 86.15% (56/65)\nClass 3: 63.79% (37/58)\nClass 4: 81.82% (45/55)\n모델 저장됨 (Validation Accuracy: 73.90%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.3431, acc=86.27%]\nEpoch 3/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.83it/s, loss=0.5127, acc=76.47%]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 결과:\nTraining Loss: 0.4929, Accuracy: 86.27%\nValidation Loss: 0.6757, Accuracy: 76.47%, F1 Score: 0.7561\n클래스별 Accuracy:\nClass 0: 80.00% (32/40)\nClass 1: 87.04% (47/54)\nClass 2: 84.62% (55/65)\nClass 3: 53.45% (31/58)\nClass 4: 78.18% (43/55)\n모델 저장됨 (Validation Accuracy: 76.47%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0637, acc=94.10%]\nEpoch 4/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.81it/s, loss=0.4344, acc=80.51%]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4 결과:\nTraining Loss: 0.2501, Accuracy: 94.10%\nValidation Loss: 0.6475, Accuracy: 80.51%, F1 Score: 0.7952\n클래스별 Accuracy:\nClass 0: 72.50% (29/40)\nClass 1: 87.04% (47/54)\nClass 2: 89.23% (58/65)\nClass 3: 63.79% (37/58)\nClass 4: 87.27% (48/55)\n모델 저장됨 (Validation Accuracy: 80.51%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0571, acc=97.42%]\nEpoch 5/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.82it/s, loss=0.6233, acc=81.99%]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5 결과:\nTraining Loss: 0.1199, Accuracy: 97.42%\nValidation Loss: 0.6465, Accuracy: 81.99%, F1 Score: 0.8100\n클래스별 Accuracy:\nClass 0: 72.50% (29/40)\nClass 1: 92.59% (50/54)\nClass 2: 86.15% (56/65)\nClass 3: 65.52% (38/58)\nClass 4: 90.91% (50/55)\n모델 저장됨 (Validation Accuracy: 81.99%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0424, acc=99.35%]\nEpoch 6/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.79it/s, loss=0.5711, acc=81.99%]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6 결과:\nTraining Loss: 0.0497, Accuracy: 99.35%\nValidation Loss: 0.7050, Accuracy: 81.99%, F1 Score: 0.8115\n클래스별 Accuracy:\nClass 0: 80.00% (32/40)\nClass 1: 90.74% (49/54)\nClass 2: 84.62% (55/65)\nClass 3: 62.07% (36/58)\nClass 4: 92.73% (51/55)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0169, acc=99.82%]\nEpoch 7/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.80it/s, loss=0.7734, acc=80.88%]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7 결과:\nTraining Loss: 0.0286, Accuracy: 99.82%\nValidation Loss: 0.7580, Accuracy: 80.88%, F1 Score: 0.8012\n클래스별 Accuracy:\nClass 0: 70.00% (28/40)\nClass 1: 94.44% (51/54)\nClass 2: 86.15% (56/65)\nClass 3: 70.69% (41/58)\nClass 4: 80.00% (44/55)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0236, acc=99.63%]\nEpoch 8/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.82it/s, loss=0.8827, acc=82.72%]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8 결과:\nTraining Loss: 0.0228, Accuracy: 99.63%\nValidation Loss: 0.7812, Accuracy: 82.72%, F1 Score: 0.8163\n클래스별 Accuracy:\nClass 0: 62.50% (25/40)\nClass 1: 92.59% (50/54)\nClass 2: 83.08% (54/65)\nClass 3: 77.59% (45/58)\nClass 4: 92.73% (51/55)\n모델 저장됨 (Validation Accuracy: 82.72%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0105, acc=99.82%] \nEpoch 9/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.81it/s, loss=0.4008, acc=82.72%]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9 결과:\nTraining Loss: 0.0141, Accuracy: 99.82%\nValidation Loss: 0.7828, Accuracy: 82.72%, F1 Score: 0.8201\n클래스별 Accuracy:\nClass 0: 72.50% (29/40)\nClass 1: 92.59% (50/54)\nClass 2: 87.69% (57/65)\nClass 3: 72.41% (42/58)\nClass 4: 85.45% (47/55)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10 [Train]: 100%|██████████| 68/68 [01:10<00:00,  1.03s/it, loss=0.0056, acc=99.91%] \nEpoch 10/10 [Val]: 100%|██████████| 17/17 [00:04<00:00,  3.78it/s, loss=0.4568, acc=83.09%]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10 결과:\nTraining Loss: 0.0102, Accuracy: 99.91%\nValidation Loss: 0.8066, Accuracy: 83.09%, F1 Score: 0.8219\n클래스별 Accuracy:\nClass 0: 80.00% (32/40)\nClass 1: 94.44% (51/54)\nClass 2: 86.15% (56/65)\nClass 3: 62.07% (36/58)\nClass 4: 92.73% (51/55)\n모델 저장됨 (Validation Accuracy: 83.09%)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\n# 예: BioBERT 기반 5-class 분류 모델\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'dmis-lab/biobert-base-cased-v1.1',\n    num_labels=5  # 분류할 클래스 수에 맞게 수정\n)\nmodel.load_state_dict(torch.load('/kaggle/working/best_biobert_model.pt'))\nmodel.to('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:58:20.979136Z","iopub.execute_input":"2025-05-20T08:58:20.979734Z","iopub.status.idle":"2025-05-20T08:58:22.522328Z","shell.execute_reply.started":"2025-05-20T08:58:20.979711Z","shell.execute_reply":"2025-05-20T08:58:22.521519Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"model.eval()\n\n# 4. 예시 추론\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\ninputs = tokenizer(\"Patients with malnutrition, alcoholism, inflammatory bowel disease, and malabsorption syndromes are at an increased risk of zinc deficiency.\", return_tensors=\"pt\").to(\"cuda\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    probs = torch.softmax(outputs.logits, dim=1)\n    pred_class = torch.argmax(probs, dim=1)\n    print(\"예측 클래스:\", pred_class.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:03:51.431729Z","iopub.execute_input":"2025-05-20T09:03:51.432011Z","iopub.status.idle":"2025-05-20T09:03:52.319156Z","shell.execute_reply.started":"2025-05-20T09:03:51.431990Z","shell.execute_reply":"2025-05-20T09:03:52.318409Z"}},"outputs":[{"name":"stdout","text":"예측 클래스: 3\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}