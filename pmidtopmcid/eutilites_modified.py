import os
import json
import pandas as pd
from Bio import Entrez, Medline
from Crawling.entrez_config import Entrez

def get_search_query_and_count(email: str, api_key: str, query:str = None) -> tuple[str, int]:
    """
    PubMed ê²€ìƒ‰ì–´ë¥¼ ë°›ì•„ ê²°ê³¼ ê°œìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤.
    FIXED_QUERYê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ì“°ê³ , ì—†ìœ¼ë©´ ì‚¬ìš©ì ì…ë ¥ì„ ìš”ì²­í•œë‹¤.

    Args:
        email (str): Entrez APIì— ì‚¬ìš©í•  ì´ë©”ì¼
        api_key (str): NCBI API í‚¤

    Returns:
        tuple: (ê²€ìƒ‰ì–´, ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜)
    """
    Entrez.email = email
    Entrez.api_key = api_key

    # ğŸ” query ì„¤ì • ë°©ì‹ ê²°ì •
    if query is not None:
        query = query
        print(f"ğŸ“Œ ê³ ì • ì¿¼ë¦¬ ì‚¬ìš©: {query}")
    else:
        query = input("ğŸ” PubMedì—ì„œ ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not query:
            print("â— ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
            exit()

    # ğŸ” ê²€ìƒ‰ ìš”ì²­
    handle = Entrez.esearch(db="pubmed", term=query, retmax=0)
    result = Entrez.read(handle)
    handle.close()

    total_count = int(result["Count"])
    return query, total_count

def run_eutilities(query: str | None = None):
    query, total_count = get_search_query_and_count(Entrez.email, Entrez.api_key, query)
    print(f"\nğŸ“„ ì´ {total_count:,}ê°œì˜ ê²°ê³¼ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")

    confirm = input("ğŸ“¥ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
    if confirm != "y":
        print("âŒ ë‹¤ìš´ë¡œë“œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
        return

    max_by_site = 10000
    max_count = min(max_by_site, total_count)
    retmax = input(f"ëª‡ ê°œì˜ ë…¼ë¬¸ì„ ê°€ì ¸ì˜¬ê¹Œìš”? (ìµœëŒ€ {max_count}ê°œ): ").strip()
    retmax = int(retmax) if retmax.isdigit() else 10
    retmax = min(retmax, max_count)

    search_handle = Entrez.esearch(db="pubmed", term=query, retmax=retmax)
    search_results = Entrez.read(search_handle)
    search_handle.close()
    id_list = search_results["IdList"]

    fetch_handle = Entrez.efetch(db="pubmed", id=",".join(id_list), rettype="medline", retmode="text")
    records = list(Medline.parse(fetch_handle))
    fetch_handle.close()

    data = [{
        "pmid": r.get("PMID", ""),
        "title": r.get("TI", ""),
        "abstract": r.get("AB", ""),
        "pub_date": r.get("DP", ""),
        "journal": r.get("JT", ""),
        "authors": ", ".join(r.get("AU", []))
    } for r in records]

    save_dir = os.path.join("Database", "eutilities")
    os.makedirs(save_dir, exist_ok=True)

    filename_base = query.replace(" ", "_")[:30]
    json_path = os.path.join(save_dir, f"{filename_base}.json")
    csv_path = os.path.join(save_dir, f"{filename_base}.csv")

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    pd.DataFrame(data).to_csv(csv_path, index=False, encoding="utf-8-sig")

    print(f"\nâœ… ì €ì¥ ì™„ë£Œ!\nğŸ“ JSON: {json_path}\nğŸ“ CSV: {csv_path}")

    # ğŸ”„ PMID to PMCID ë§¤í•‘ ë° full text ìˆ˜ì§‘
    from tqdm import tqdm
    import requests
    import time

    def convert_pmid_to_pmcid(pmid):
        url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        params = {
            "db": "pmc",
            "term": f"{pmid}[pmid]",
            "retmode": "json",
            "api_key": Entrez.api_key
        }
        try:
            res = requests.get(url, params=params)
            res.raise_for_status()
            data = res.json()
            idlist = data.get("esearchresult", {}).get("idlist", [])
            return idlist[0] if idlist else None
        except Exception as e:
            print(f"âŒ PMID {pmid} â†’ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None

    def fetch_fulltext_by_pmcid(pmcid):
        try:
            handle = Entrez.efetch(db="pmc", id=pmcid, rettype="full", retmode="xml")
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(handle.read(), "lxml")
            handle.close()
            body = soup.find("body")
            if body:
                paras = body.find_all("p")
                return "\n".join(p.get_text(strip=True) for p in paras)
        except Exception as e:
            print(f"âŒ PMCID {pmcid} â†’ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return "âŒ Full text not available"

    enriched_data = []
    for item in tqdm(data, desc="ğŸ”„ PMID â†’ PMCID â†’ Full Text"):
        pmid = item["pmid"]
        pmcid = convert_pmid_to_pmcid(pmid)
        item["pmcid"] = pmcid if pmcid else "N/A"
        item["full_text"] = fetch_fulltext_by_pmcid(pmcid) if pmcid else "âŒ No PMCID"
        enriched_data.append(item)
        time.sleep(0.34)

    # ë‹¤ì‹œ ì €ì¥
    enriched_json_path = os.path.join(save_dir, f"{filename_base}_full.json")
    enriched_csv_path = os.path.join(save_dir, f"{filename_base}_full.csv")

    with open(enriched_json_path, "w", encoding="utf-8") as f:
        json.dump(enriched_data, f, ensure_ascii=False, indent=2)

    pd.DataFrame(enriched_data).to_csv(enriched_csv_path, index=False, encoding="utf-8-sig")
    print(f"ğŸ“¥ ë³¸ë¬¸ í¬í•¨ ì €ì¥ ì™„ë£Œ!\nğŸ“ JSON: {enriched_json_path}\nğŸ“ CSV: {enriched_csv_path}")